# -*- coding: utf-8 -*-
"""batch_spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xjqB9DZAqCY4MCAiHbCHf88nAXDC5PB5
"""

import findspark
import requests
import pandas as pd
import zipfile
import findspark
import requests
import pandas as pd
import zipfile
import os
from google.cloud import storage


findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, monotonically_increasing_id, explode_outer
from pyspark.sql.types import StructType, StructField, StringType, ArrayType
from concurrent.futures import ThreadPoolExecutor

spark = SparkSession.builder.appName('Batch').getOrCreate()

# URL del archivo ZIP a descargar
url = 'https://www.dtpm.cl/descargas/gtfs/GTFS-V122-PO20240427.zip'

# Ruta donde guardar el archivo ZIP localmente
local_zip_file_path = '/tmp/archivo.zip'

# Descarga el archivo ZIP con requests
response = requests.get(url)
with open(local_zip_file_path, 'wb') as f:
    f.write(response.content)

# Ruta del bucket de Google Cloud Storage donde se guardarÃ¡n los archivos
bucket_name = 'datos-red'

# Ruta del archivo ZIP en el bucket
bucket_zip_file_path = 'archivo.zip'

# Carga el archivo ZIP al bucket de Google Cloud Storage
client = storage.Client()
bucket = client.bucket(bucket_name)
blob = bucket.blob(bucket_zip_file_path)
blob.upload_from_filename(local_zip_file_path)

# Descomprime el archivo ZIP directamente en Google Cloud Storage
with zipfile.ZipFile(local_zip_file_path, 'r') as zip_ref:
    for file_info in zip_ref.infolist():
        filename = file_info.filename
        blob = bucket.blob(f'datos/{filename}')
        with zip_ref.open(filename) as file:
            blob.upload_from_file(file)

# Lee los archivos de las carpetas descomprimidas en Google Cloud Storage
folder_path = 'gs://{}/'.format(bucket_name)

# Lista de archivos en el directorio
file_list = [file.name for file in client.list_blobs(bucket_name, prefix='datos/')]

for file in file_list:
    if file.endswith('.txt'):
        # Lee el archivo de texto en un DataFrame de PySpark
        df = spark.read.text(folder_path + file)
        final_df = df.coalesce(1)

        # Escribe el DataFrame a un archivo CSV
        csv_file = file.replace('.txt', '.csv')
        final_df.write.csv(folder_path + csv_file, mode='overwrite', header=True)

        # Elimina el archivo TXT
        blob = bucket.blob(file)
        blob.delete()