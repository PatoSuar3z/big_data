# -*- coding: utf-8 -*-
"""realtime_spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yY1QoJcriFUJz23ALymA4n2U_xdkgvcH
"""

import findspark
import requests
import pandas as pd

findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, monotonically_increasing_id, explode_outer
from pyspark.sql.types import StructType, StructField, StringType, ArrayType
from concurrent.futures import ThreadPoolExecutor

spark = SparkSession.builder.appName('Test').getOrCreate()

import requests
import json

# URL del JSON
url = "https://www.red.cl/restservice_v2/rest/getparadas/all"

# Realizar la solicitud GET para obtener el JSON
response = requests.get(url)

# Verificar si la solicitud fue exitosa (cÃ³digo de estado 200)
if response.status_code == 200:
    # Guardar el contenido de la respuesta en un archivo
    with open("paradas.json", "wb") as f:
        f.write(response.content)
    print("El JSON ha sido descargado y guardado correctamente en paradas.json.")
else:
    print("Error al descargar el JSON. CÃ³digo de estado:", response.status_code)

base_url = "https://www.red.cl/predictor/prediccion"

t="eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE3MTU0MDE3ODIxMzN9.Onc7Thfc66rAHpnv_WiurW5AgiF8eZI2tzZawNI6ZlA"

# Cargar el JSON de los cÃ³digos de recorrido desde el archivo
with open("paradas.json", "r") as f:
    paradas_data = json.load(f)

# Asignar la lista de cÃ³digos de recorrido al nombre de variable adecuado
codigos_recorrido = paradas_data

from functools import reduce

schema = StructType([
    StructField("fechaprediccion", StringType(), True),
    StructField("horaprediccion", StringType(), True),
    StructField("nomett", StringType(), True),
    StructField("paradero", StringType(), True),
    StructField("respuestaParadero", StringType(), True),
    StructField("servicios", StructType([
        StructField("item", ArrayType(StructType([
            StructField("codigorespuesta", StringType(), True),
            StructField("distanciabus1", StringType(), True),
            StructField("distanciabus2", StringType(), True),
            StructField("horaprediccionbus1", StringType(), True),
            StructField("horaprediccionbus2", StringType(), True),
            StructField("ppubus1", StringType(), True),
            StructField("ppubus2", StringType(), True),
            StructField("respuestaServicio", StringType(), True),
            StructField("servicio", StringType(), True),
            StructField("color", StringType(), True),
            StructField("destino", StringType(), True),
            StructField("sentido", StringType(), True),
            StructField("itinerario", StringType(), True),
            StructField("codigo", StringType(), True)
        ])), True)
    ]), True),
    StructField("urlLinkPublicidad", StringType(), True),
    StructField("urlPublicidad", StringType(), True),
    StructField("x", StringType(), True),
    StructField("y", StringType(), True)
])

# Lista para almacenar los DataFrames resultantes
dataframes = []

def fetch_data(codigo):
    # ParÃ¡metros de la solicitud
     params = {'t': t,'codsimt': codigo, 'codser': ""}

    # Realizar la solicitud GET a la API
    response = requests.get(base_url, params=params)

    # Verificar si la solicitud fue exitosa (cÃ³digo de estado 200)
    if response.status_code == 200:
        # Convertir la respuesta JSON en un DataFrame de Spark utilizando el esquema definido
        return spark.createDataFrame([response.json()], schema=schema)
    else:
        print(f"Error al obtener datos para el cÃ³digo de recorrido {codigo}")
        return None

# Utilizar ThreadPoolExecutor para enviar mÃºltiples solicitudes de manera simultÃ¡nea
with ThreadPoolExecutor(max_workers=1000) as executor:
    # Obtener los resultados de las solicitudes en paralelo
    dataframes = list(executor.map(fetch_data, codigos_recorrido))

# Filtrar los resultados para eliminar los elementos nulos
dataframes = [df for df in dataframes if df is not None]

# Unir todos los DataFrames en uno solo
merged_df = reduce(lambda x, y: x.union(y), dataframes)

# Expandir la columna 'servicios' en el DataFrame
expanded_df = merged_df.withColumn("servicio", explode_outer(col("servicios.item")))

# Agregar un ID Ãºnico para cada Ã­tem
expanded_df = expanded_df.withColumn("item_id", monotonically_increasing_id())

# Seleccionar los atributos de 'servicio', incluyendo el nuevo ID, y aÃ±adirlos al DataFrame principal
final_df = expanded_df.select(
    "fechaprediccion",
    "horaprediccion",
    "nomett",
    "paradero",
    "respuestaParadero",
    "servicio.*",
    "x",
    "y",
    "item_id"
)


# Especifica la ruta de tu bucket en lugar de 'gs://nombre_del_bucket/'
ruta_bucket = 'gs://datos-red/'

final_df_single_partition = final_df.coalesce(1)

final_df_single_partition.write.csv(ruta_bucket + 'archivo.csv', header=True, mode='overwrite')

final_df.show()

spark.stop()